{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-iTIUU_AOIi",
        "outputId": "b66c0b86-ed63-4a4f-d1aa-42b1c9b9cd7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Collecting pytorch-pretrained-bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (1.25.2)\n",
            "Collecting boto3 (from pytorch-pretrained-bert)\n",
            "  Downloading boto3-1.34.41-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (4.66.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2023.12.25)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2.1.0)\n",
            "Collecting botocore<1.35.0,>=1.34.41 (from boto3->pytorch-pretrained-bert)\n",
            "  Downloading botocore-1.34.41-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch-pretrained-bert)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->pytorch-pretrained-bert)\n",
            "  Downloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.41->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch-pretrained-bert) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch-pretrained-bert) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.41->boto3->pytorch-pretrained-bert) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.34.41 botocore-1.34.41 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 s3transfer-0.10.0\n",
            "Collecting pytorch-nlp\n",
            "  Downloading pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-nlp) (1.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-nlp) (4.66.1)\n",
            "Installing collected packages: pytorch-nlp\n",
            "Successfully installed pytorch-nlp-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch-pretrained-bert\n",
        "!pip install pytorch-nlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "model_name = \"/content/sample_data/Saurabh\"  # Update with the correct path to your fine-tuned model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"/content/sample_data/SaurabhToken\")\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "J8J0GJxDKDNz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I unliked the movie, but it could have been much better with Mr.XYZ in casting team\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)"
      ],
      "metadata": {
        "id": "DBsWVsZdKGC0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)"
      ],
      "metadata": {
        "id": "uQkYrOikKbCy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = torch.softmax(outputs.logits, dim=-1)\n",
        "predicted_class_id = predictions.argmax().item()"
      ],
      "metadata": {
        "id": "14024Wp7LYlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Predicted class ID: {predicted_class_id}\");"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KspRgFQbLcO8",
        "outputId": "6fa5ad24-a9c4-44dc-b85f-0181d9c5ef87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class ID: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgZom8CQBZU2",
        "outputId": "93fdf2fa-8ba6-401c-a54e-c339f91c697d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "bl4Pvc4NEBhF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/sample_data/IMDB Dataset.csv\")\n",
        "df = df.head(15000)\n",
        "df.sentiment = [1 if s == 'positive' else 0 for s in df.sentiment]"
      ],
      "metadata": {
        "id": "NUWFO7A6D-ar"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train,temp = train_test_split(df, test_size=0.2, random_state=42)\n",
        "val,test=train_test_split(temp, test_size=0.5,random_state=42)"
      ],
      "metadata": {
        "id": "TeqMCNqmECYB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = train.review.values\n",
        "train_labels = train.sentiment.values\n",
        "val_sentences = val.review.values\n",
        "val_labels = val.sentiment.values\n",
        "test_sentences = test.review.values\n",
        "test_labels = test.sentiment.values\n",
        "\n",
        "train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JxfrOVUUxBzs",
        "outputId": "4b6c9a93-e719-4358-da10-b20a10b02db7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  review  sentiment\n",
              "9839   its too bad that no one knows anything about t...          1\n",
              "9680   I liked most of this film. As other reviews me...          0\n",
              "7093   Yeah, unfortunately I came across the DVD of t...          0\n",
              "11293  The plot was predictable, and fighting with gu...          0\n",
              "820    After seeing Dick Tracy in the 6.99$ bin at Fu...          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-42915780-0889-4e4e-bf0e-b4ec91f1800f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9839</th>\n",
              "      <td>its too bad that no one knows anything about t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9680</th>\n",
              "      <td>I liked most of this film. As other reviews me...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7093</th>\n",
              "      <td>Yeah, unfortunately I came across the DVD of t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11293</th>\n",
              "      <td>The plot was predictable, and fighting with gu...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>820</th>\n",
              "      <td>After seeing Dick Tracy in the 6.99$ bin at Fu...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42915780-0889-4e4e-bf0e-b4ec91f1800f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-42915780-0889-4e4e-bf0e-b4ec91f1800f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-42915780-0889-4e4e-bf0e-b4ec91f1800f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d3fceb5f-58f7-4233-801d-75509f26c8d5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d3fceb5f-58f7-4233-801d-75509f26c8d5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d3fceb5f-58f7-4233-801d-75509f26c8d5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train",
              "summary": "{\n  \"name\": \"train\",\n  \"rows\": 12000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"Given the opposite circumstance of 2009 where the reality is we do have a black president, this movie takes on quite a powerful historical significance. For entertainment value I found this movie to be both engaging and repugnant. I was quite taken back of course by the blatant racism of the time, but also found the music and dancing incredible. Also it is quite cool to see Sammy Davis Jr as such a very young child actor. He plays Rufus Jones, a young boy who is being consoled by his Mammy. He is told 'Why some day you could be President'. This was so ridiculous in 1933 that it was mocked and thought to be endearing, charming and funny. The bulk of the movie is a fantasy sequence of what the government would be like if it was run by a black man. They depict the seats of government as being like a revivalist Baptist church.<br /><br />The fact was when I stumbled onto this movie one day it drew me in. It is really well done and very entertaining. I believe if we can look beyond the racism we can see this movie for all it brings us. In fact to realize that it is not only not ridiculous to have a black president, but that it is normal, just makes this movie that much more relevant. It clearly marks a moment in time for our collective consciousness.\",\n          \"\\\"Girlfight\\\" is much more of a coming-of-age-story than it is a fight flick. And what a relief to have one in an urban school, with naturalistic, realistic Latinos and believable use of Brooklyn project settings. <br /><br />It made me realize that virtually all Hollywood high school movies are set in luxurious suburbia or small towns. (Even the somewhat comparable \\\"Love and Basketball\\\" which focused on teen African-Americans was set in suburbia.) While these kids share some of the same peer problems, those issues shrink compared to the other struggles of these kids, where high school graduation could be the major accomplishment of their lives.<br /><br />The feminist element here is riveting in its originality, as you hold your breath to see if she can have a relationship--and a victory-- on her terms. A lots of audience sympathy goes to the guy who is challenged to rise to a gender-bending-expectations situation.<br /><br />The movie does drag a bit here and there, but this is no cheap thrills \\\"Rocky\\\" fight movie, as the practices and fights have complex outcomes, and all the relationships--especially with fathers and father-figures-- take more center stage than the center ring. <br /><br />There were lots of interesting music credits listed at the end, but I hadn't really noticed the songs.<br /><br />(originally written 10/7/2000)\",\n          \"I attended Camp Chesapeake. It was located at the head of the Chesapeake bay on the North East River in MD. It was a similar type summer camp with cabins. It was established by the Coatesville, PA YMCA. I started out as a young camper and later became a Junior, Senior counselor and later, the Waterfront director. If the camp had continued, I would have done anything within my power to become the camp director. Alas the powers of the YMCA decided to close down the camp and sell it to the state of MD. I visited the former camp some years later by boat and was dismayed by the neglect of the state of MD and natural destruction by mother nature. The 350 acre site served so many with all the benefits of contact with natures offerings. A black man by the name of Curtis Ford, and his family were residents and caretakers of the property. Mr Curtis was my friend and mentor. I idolized his every being. Even as he could not swim he was a waterman. If I asked him where the fish were biting, he would designate the spot, and I would have a ball. Ther was also a Family camp at the end of the summer. These memories will be with me for eternity.\"\n        ],\n        \"num_unique_values\": 11979,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"num_unique_values\": 2,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted=[]\n",
        "actual=[]"
      ],
      "metadata": {
        "id": "4qiA3KZ-6NU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "\n",
        "openai.api_key = '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'\n",
        "\n",
        "def predict(text):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are my helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Predict the sentiment of this review: \\\"{text}\\\". Is it positive or negative?\"}\n",
        "        ]\n",
        "    )\n",
        "    prediction = response.choices[0].message['content'].strip().lower()\n",
        "    if \"positive\" in prediction:\n",
        "        return 'positive'\n",
        "    else:\n",
        "        return 'negative'\n",
        "count=1\n",
        "for i in range(0,50):\n",
        "  try:\n",
        "    review = test.iloc[i]['review']\n",
        "    predicted_sentiment = predict(review)\n",
        "    actual_sentiment = test.iloc[i]['sentiment'].lower()\n",
        "    predicted.append(predicted_sentiment)\n",
        "    actual.append(actual_sentiment)\n",
        "    print(\"review\",review)\n",
        "    print(f\"Predicted Sentiment: {predicted_sentiment}, Actual Sentiment: {actual_sentiment}\")\n",
        "    count+=i\n",
        "    if(count%2==0):\n",
        "      sleep(20)\n",
        "  except:\n",
        "    sleep(20)\n",
        "    continue\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8FoH0W-Gg3n",
        "outputId": "f7d07b3b-fc8e-4bdf-9e3f-ab5bb5cdd046"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "review After reading the novel which is about a one hour read, watching this film became a sad disappointing experience. Just as he did in prince of Egypt simon wells somehow managed to direct a script that took away all the drama and mystery out of its source material and turned it into this homogenized nonsense. Now I'm a sucker for cheese and camp but this movie made absolutely no sense. There was no joy in any of the performances or any humor. There were no thrills and that silly bookend with addy's character of filby throwing his hat in the air was the last hackwriting straw. I felt very violated when this movie was over and I still refuse to believe it was only 90 minutes it went on forever. I wondered how the studio and director could have OK'd such a lousy script but then my friend pitched the movie to me exactly as It was and I said wow that sounds great but what happened to the movie.\n",
            "Predicted Sentiment: negative, Actual Sentiment: negative\n",
            "review Rowan Atkinson delivers an unforgettable performance as the clueless Mr. Bean who never goes far without his Teddy Bear. The appeal of Mr. Bean is largely his childish behavior and innocence. We don't know if he came from the sky or another planet. He is the kind of strange character that you can't make up quite easily. He is often alone and used to it. He has a hard time communicating through speech which might be why we only hear his grunts at times. There are other characters who speak to him and he responds. The character of Mr. Bean is a mystery and still is. He lives alone and does the unthinkable when he can do the sensible thing. Mr. Bean is rather an odd man out who does not mind it much. He rather live a simple life with his yellow car and teddy bear and hopes to get to work on time.\n",
            "Predicted Sentiment: positive, Actual Sentiment: positive\n",
            "review This movie is just great... It starts out real slow and boring but as the movie progresses.....well the fun keeps on coming. The power of the movie is perhaps in it's subtle references to a lot (and I mean a lot) of other movies. For me the best part was perhaps the Bruce Lee/Crouching Tiger, Hidden Dragon scene. How the two actors switches from French to Chinese or whatever Asian language it was, it was awesome. Then the jokes with the names, they are hilarious but perhaps you won't understand all of them when your an American (no offense). Overall a great movie\n",
            "Predicted Sentiment: positive, Actual Sentiment: positive\n",
            "review What on earth? Like watching an episode of Neighbours after drinking two bottles of cough medicine- nightmarish and making no sense at all. I was waiting for the clever part where everything fits into place and saves the film. Maybe it was there and i just missed it, or was lost on me.<br /><br />My strongest suspicion is that it is a thinly veiled attempt to market a new drug thats about to hit the streets. I wouldn't say \"don't watch it\" but I will say its pretty poor on every level- like am dram in high def. Whack. Unless you drink two bottles of cough syrup. Then it's just dandy.\n",
            "Predicted Sentiment: negative, Actual Sentiment: negative\n",
            "review This had the promise of being an interesting film. The subject matter was certainly a promising one - the excesses of the Catholic Church during the counter-reformation. However, not only was this not developed (other than a two paragraph introduction), many things were not explained - i.e. the gypsies, the Anabaptists, the inquisitors and their relationship to the one true church. Nor were the politics of the time explained, i.e. the relationship between the Catholic church and its supporters like the Holy Roman Emperor. Though these may have been apparent to an Austrian audience, the lack of explanation makes it confusing for Americans.<br /><br />But perhaps it's a good thing that they didn't emphasize the history since what they showed was pretty inaccurate anyway. Instruments of torture, bloody executions, witch and heretic burnings, big shiny swords and pretty golden reliquaries are the stars of the film. It could have just as easily been one of those Conan-type sword and sorcery movies, only with period costumes...\n",
            "Predicted Sentiment: negative, Actual Sentiment: negative\n",
            "review Oh, man, I hated this movie. Granted, the site locations were great, but that's about the ONLY positive thing I can say about it. Now, I'm going to state right at the beginning that I am VERY critical of the way weapons, especially firearms, are both portrayed, and handled, in movies. Being a war flick, portrayal was fine, but the shoddy weapons handling in the movie would have NEVER been tolerated by a real SEAL Team. The acting was more wooden than my first sailboat, the equipment carried (or lack of it) was laughable, and the dialogue was, shall we say, lacking in ANYTHING interesting. Well, with the exception of the journalist, which was actually prescient. Watching this movie was comparable to watching \"Palmetto\" with Woody Harrelson, where each scene was so bad you just couldn't turn it off, because you had to see if they could get worse with the next scene. Like Palmetto, they certainly did. The scene in the water, where, after shooting the first of the enemy, they BLOW THE DAMN BOAT UP, thereby having to face possible drowning, made me laugh so hard, that for a millisecond, I almost thought it was worth waiting through the movie for. Then Charlie Sheen decided to drag the surviving enemy down to the depths of the ocean (the way it was filmed, with the many camera cuts, it looked like they went down about 80 feet. Nice continuity there....) before slashing his throat was so damn stupid, I was stunned. Then again, so was the whole damn movie. I enjoy action movies, but not this one. NOTE: The version I watched was a TV version, pan & scan. I can't imagine that made a difference, except for making the whole thing blessedly shorter!\n",
            "Predicted Sentiment: negative, Actual Sentiment: negative\n",
            "review This is the worst movie I have ever seen. The Avengers held this dubious honor but no longer. The acting in \"Jill the Ripper\" is terrible and was only eclipsed by the plot. This movie is as intellectually stimulating as the Telletubbes. It doesn't know whether it wants to be an S&M flick or a really bad thriller. Only watch under extreme intoxication or if you're bed ridden and need a leather clad distraction. This script should be reworked into a porn, it wouldn't take very much effort and would have a longer shelf life. A porn, even a bad porn, wouldn't do the damage to Dolf Lundgren's career the way that this movie has.\n",
            "Predicted Sentiment: negative, Actual Sentiment: negative\n",
            "review Hey now, I have never laid eyes on a Manga comic, but apparently this movie is based on one. Ah well, such is life. Anyway, this is a pretty bizarre, to say the least, movie, as things literally spiral out of control in a small Japanese town. People are becoming obsessed with the uzumaki (spiral) and this young girl watches her friend's father videotape a snail, and later in the movie, people start becoming snails? It also seems this boy's dad becomes so obsessed he somehow commits suicide in a household appliance. There is some bizarre humor here that might be at home in a Tim Burton movie, but there is some nastiness & gore like only the Japanese can do with justice. As with a lot of Japanese films though, the ending is the ending, and did anything get resolved? Well, not to my mind, it didn't. There are some hints as to why this is all happening but they aren't explored and there's a lot left to either the imagination or else it wasn't deemed important. Still though, there are lots of things for the eye to feast on and if you aren't obsessed with everything making perfect sense, this is well worth seeing, because it's just so original and bizarre. My favorite was the father with the spinning eyeballs, personally. 8 out of 10 stars.\n",
            "Predicted Sentiment: positive, Actual Sentiment: positive\n",
            "review Ah yes the 1980s , a time of Reaganomics and Sly , Chuck and a host of other action stars hiding in a remote jungle blowing away commies . At the time I couldn`t believe how movies like RAMBO , MISSING IN ACTION and UNCOMMON VALOR ( And who can forget the ridiculous RED DAWN ? ) made money at the box office , they`re turgid action crap fests with a rather off putting right wing agenda and they have dated very badly . TROMA`S WAR is a tongue in cheek take on these type of movies but you`ve got to ask yourself did they need spoofing in the first place ? Of course not . TROMA`S WAR lacks any sort of sophistication - though it does make the point that there`s no real difference between right wing tyrants and left wing ones - and sometimes feels more like a grade z movie than a send up . Maybe it is ?\n",
            "Predicted Sentiment: negative, Actual Sentiment: negative\n",
            "review Pure crap, decent cinematography... I liked some of colors. Other than that, this was one of the worst movies I ever saw. Boring, lifeless, not once did I find myself interested in any of the characters. I kept waiting for a real plot to form and the movie to pick up the pace. Nothing ever happened! I think they spent too much time working on hair and wardrobe that they forgot there was a movie being made at the time.\n",
            "Predicted Sentiment: negative, Actual Sentiment: negative\n",
            "review Blake Edwards' legendary fiasco, begins to seem pointless after just 10 minutes. A combination of The Eagle Has Landed, Star!, Oh! What a Lovely War!, and Edwards' Pink Panther films, Darling Lili never engages the viewer; the aerial sequences, the musical numbers, the romance, the comedy, and the espionage are all ho hum. At what point is the viewer supposed to give a damn? This disaster wavers in tone, never decides what it wants to be, and apparently thinks it's a spoof, but it's pathetically and grindingly square. Old fashioned in the worst sense, audiences understandably stayed away in droves. It's awful. James Garner would have been a vast improvement over Hudson who is just cardboard, and he doesn't connect with Andrews and vice versa. And both Andrews and Hudson don't seem to have been let in on the joke and perform with a miscalculated earnestness. Blake Edwards' SOB isn't much more than OK, but it's the only good that ever came out of Darling Lili. The expensive and professional look of much of Darling Lili, only make what it's all lavished on even more difficult to bear. To quote Paramount chief Robert Evans, \"24 million dollars worth of film and no picture\".\n",
            "Predicted Sentiment: negative, Actual Sentiment: negative\n",
            "review I rarely write reviews but this film simply demands more attention than it gets as it contains the most hysterical kidnapping gone comically wrong sequence ever filmed.<br /><br />I have only seen this once and found it to be the funniest film I had ever had the privilege to watch. I laughed from beginning to end. It is such a shame it is not out on DVD or video.<br /><br />You can only compare its cinematography with that of It's A Mad, Mad, Mad, Mad World or Promise her Anything. Only this storyline isn't nearly as complex as Mad World.<br /><br />I hope in the near future this film is released as it would be a shame to lose such a comedy gem amongst the dregs we have nowadays.\n",
            "Predicted Sentiment: positive, Actual Sentiment: positive\n",
            "review I'm an atheist. To me history and truth mean a lot.<br /><br />This film is made after a novel published in 1921, which is still being updated up to this day as if it was a history book. Well it's not. The movie is about the novels 1950s version. Some actors were GREAT but that doesn't cover the plot.<br /><br />In short man invents a super-bomb so God and his friends hold a tribunal to see if they must intervene. The devil analogy persecutes man, and for defense we have the spirit of man. What is the spirit of man anyway? And why was the first defendant Adam? Eventually you just get US Christian propaganda in a 5th grade history book of the time. Though other religions are mentioned, only European Christianity is explored.<br /><br />First we get the caveman story. The women are scrawny stereotypes of damsels in distress. Real cave women were as strong as men and just as resistant. Hard times, hard life, adapt and survive. All this is watered down by mid-century stereotypes.<br /><br />Next we get Egypt's first pyramid construction. Today we see a different story and know that there were a lot less deaths and regular citizens at work as well. Loosing mentioned amount of many lives in the process would have been a national disaster and nobody after would try to beat it. As if there was only ONE pyramid build.<br /><br />The part about Moses and one true god was as if the Spanish inquisition was asking nicely. Inquisition itself was never even mentioned in the movie.<br /><br />Helen of Troy's evil grim was so vile that I didn't see why so many were even interested in her. In reality they were just soldiers, following commanders orders, who were \"discussing\" a political issue of power. She was just an excuse.<br /><br />The Cleopatra story was were I saw this film was to inaccurate and filled with propaganda. Here brother was a LOT younger. She was not obsessed with poison, was quite educated to restore library content, and was politically competitive to drag beaten down Egypt out of dirt.<br /><br />The part with Nero and praying Christians in a cave were disgusting. Yes, Rome burned down. Yes, there was persecuted Christianity. But the way they portray it was as if the Coliseum build itself and there was no Vespasian to rebuild Rome.<br /><br />Attila the Hun appears in a short seen and than we jump to King Arthur. The crusades are mentioned with minimal bloodshed. And there is no mention of the crusades east to Russia that ended in an ironic battle. The knights just went home and started jousting for fun of it. A LOT of stuff is put down like no indoor pluming, hygiene and plagues.<br /><br />Then they cover Joan of Ark, where she always has to much makeup and looks like a princes. Territorial politics were replaced with an unjust court. The sidesaddle alone on a stool makes me want to ask how someone could follow here. At here burning I wanted to yell \"Hura! Now die already! Cheap special effects, where is the fire?\".<br /><br />By the time they mentioned Leonardo I already got fed up with the movie. Columbus, Spanish slaughter of America, yelling Queen Elisabeth \"kick the Spanish armada\" and so on and so on.<br /><br />The ONLY reason I wanted to see this movie was the fact that it was the last one with all 3 Marks brothers. And all they got was the scene with Manhattan and Indians. Amusing, but no more than a smile.<br /><br />The witch-hunts are mentioned briefly, as well as plagues (after renascence). When they start portraying revolutions, things gut power-hungry and anarchistic. The US revolution was pursued by the French revolution. Oppression and incompetence are bad, but you can't just blow the old way up out of anger, you must replace it with something. So they replaced the French monarchy with new French monarchy. So we get Napoleon and his ambitions to go to India by land. But they replace his motives with unity and band him for only the title \"Emperor\". The conquests in Europe, defeat in Russia are sacked to Waterloo.<br /><br />The US civil war, the English rich inventors (Tesla not included). \"Mister Watson, come here, I want you\" almost made me laugh for teenage reasons. Technological hard work was watered down to the final discovery and comedic misuse.<br /><br />Eventually after 85 minutes we come to world wars and organized crime, but none of its horrors. Adolph's words \"I invade Russia. This is my last territorial demand\" were hilarious. It was his LAST territorial demand.<br /><br />To build suspense God puts a countdown clock to doomsday on the \"wall\" for the final words. All mighty cant pause the universe for a second? There was no need for the persecution speech but the defense made one last throw.<br /><br />Last we see the man of tomorrow as the final defense. Apparently a paradox man, because the bomb was to go of today. His toys are a music box in the shape of a gun and a pencil box sword. Now that is so wrong Pens and pencils drew so many weapon blueprints that its kill count surpasses the atom bomb. And making music out of a weapon? Deluded egoistic generals make music out of weapon fire. So the man of tomorrow is already a monster.<br /><br />The way I see it, all the defense had to do was blame the devil as the true conspirator for mans demise and case closed. And honestly, compared to all barbaric stuff our ancestors did centuries ago we are pretty humane at painless backstabbing these days.<br /><br />To summarize all I will just quote \"Firefly\"s episode \"Jaynestown\": \"It's my estimation that every man ever got a statue made of him was one kind of son of bitch or another. Ain't about you, Jayne. It's about what they need\".\n",
            "Predicted Sentiment: negative, Actual Sentiment: negative\n",
            "review Ingrid Bergman, playing dentist Walter Matthau's faithful receptionist who harbors a little crush on her boss, is absolutely wonderful in this film. She handles the witty repartee in the script with aplomb and steals a terrific scene where she and Goldie Hawn talk in a record booth (Ingrid's monologue is a front, but her face tells you she believes in it with all her heart). Matthau is an odd choice for the leading man (he's too old for Goldie Hawn and too unrefined for Bergman, not to mention too unfocused to be a dentist), but I liked the way he tries hard to please Goldie and stumbles around trying to free himself from a lie. Hawn (who won a Supporting Oscar) is just as fresh and bubbly as she is today. This bedroom farce isn't terribly sophisticated (and faintly reminds one of \"Any Wednesday\" besides), but it's a welcome relief from the noisy, teen-oriented comedies they turn out today. \"Cactus Flower\" is a lovely sigh! *** from ****\n",
            "Predicted Sentiment: positive, Actual Sentiment: positive\n",
            "review Whenever someone tries to tell me that they think a movie is the worst ever (and it's usually some movie that's \"cool\" to hate, like \"Manos, the Hands of Fate\" or \"The Avengers\") I ask them, \"is that movie a comedy about an orphan who is constantly trying to murder adults? Does anyone utter the line 'I'd rather eat a turd' in that movie?\"<br /><br />This movie is WAY too infantile and moronic for adults, and WAY too violent and irresponsible for children. Is there that much money in the Beavis and Butt-head demographic to make a series of movies like this? There is a Problem Child 3, but I haven't seen it. I'd rather eat a turd.\n",
            "Predicted Sentiment: negative, Actual Sentiment: negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(predicted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUmJ6vUO6CMd",
        "outputId": "ac961c53-8b3d-49ca-e14b-c46da5442e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train= train.to_dict(orient='records')\n",
        "val= val.to_dict(orient='records')\n",
        "test = test.to_dict(orient='records')\n",
        "type(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFaDGy-dtco5",
        "outputId": "8560321e-81c5-4ac5-8e0a-8d7a4f8cd67b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, train_labels = list(zip(*map(lambda d: (d['review'], d['sentiment']), train)))\n",
        "val_texts, val_labels = list(zip(*map(lambda d: (d['review'], d['sentiment']), val)))\n",
        "test_texts, test_labels = list(zip(*map(lambda d: (d['review'], d['sentiment']), test)))\n",
        "\n",
        "len(train_texts), len(train_labels), len(val_texts), len(val_labels) ,len(test_texts), len(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs3kuheDuD2c",
        "outputId": "e610cb59-8c2b-412e-b65b-bbc91e538822"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12000, 12000, 1500, 1500, 1500, 1500)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], train_texts))\n",
        "val_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], val_texts))\n",
        "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], test_texts))\n",
        "\n",
        "len(train_tokens), len(val_tokens), len(test_tokens)"
      ],
      "metadata": {
        "id": "Vt1I8djmuGGS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92c2267d-c505-4370-fd62-6ea219fc3e8f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12000, 1500, 1500)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "NtaZnuslv9RQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "val_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, val_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "test_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "\n",
        "train_tokens_ids.shape,val_tokens_ids.shape, test_tokens_ids.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ziL1XyguNpN",
        "outputId": "57270fa1-6a99-4454-bc3d-97b5abdb6742"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((12000, 512), (1500, 512), (1500, 512))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
        "val_masks = [[float(i > 0) for i in ii] for ii in val_tokens_ids]\n",
        "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]"
      ],
      "metadata": {
        "id": "Gnu8fV9QuUS_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(data,labels):\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  for sent in data:\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          sent,\n",
        "                          add_special_tokens = True,\n",
        "                          max_length = 512,\n",
        "                          pad_to_max_length = True,\n",
        "                          return_attention_mask = True,\n",
        "                          return_tensors = 'pt',\n",
        "                    )\n",
        "\n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "  return input_ids, attention_masks, labels"
      ],
      "metadata": {
        "id": "RcumxLdRuXzP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_ids, train_attention_masks,train_labels = generate_data(train_sentences,train_labels)\n",
        "val_input_ids, val_attention_masks,val_labels = generate_data(val_sentences,val_labels)\n",
        "test_input_ids, test_attention_masks,test_labels = generate_data(test_sentences,test_labels)\n",
        "\n",
        "print('Original: ', train_sentences[1])\n",
        "print('Token IDs:', train_input_ids[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOZieDqsucWG",
        "outputId": "6287972f-c5fc-4ac4-d5bf-ff9a91dfdbf7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  I liked most of this film. As other reviews mentioned it has a good cast, the plot is interesting enough. All in all it is fun to watch.<br /><br />But the ending, I feel, is completely botched, it left me bewildered. Yes, you expect people crossing and double-crossing each other in this sort of movie, but quadruple-crossing? Well, if it's justified by the plot then why not? <br /><br />But that's the bad part, there's completely no need for it. After a certain point it's all scheming with completely no meaning. (here comes the SPOILER). After the airport scene Enrico and his accomplices already HAVE the money. I couldn't understand the need for the rest of the scam. Is it all necessary just to rub Federico's nose in the fact that he's been fooled? I don't buy it.<br /><br />So 6 out of 10 for 3/4 of the film and 2 out of 10 for the ending.\n",
            "Token IDs: tensor([  101,  1045,  4669,  2087,  1997,  2023,  2143,  1012,  2004,  2060,\n",
            "         4391,  3855,  2009,  2038,  1037,  2204,  3459,  1010,  1996,  5436,\n",
            "         2003,  5875,  2438,  1012,  2035,  1999,  2035,  2009,  2003,  4569,\n",
            "         2000,  3422,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,\n",
            "         1028,  2021,  1996,  4566,  1010,  1045,  2514,  1010,  2003,  3294,\n",
            "        28516,  7690,  1010,  2009,  2187,  2033, 24683,  1012,  2748,  1010,\n",
            "         2017,  5987,  2111,  5153,  1998,  3313,  1011,  5153,  2169,  2060,\n",
            "         1999,  2023,  4066,  1997,  3185,  1010,  2021, 17718, 21531,  2571,\n",
            "         1011,  5153,  1029,  2092,  1010,  2065,  2009,  1005,  1055, 15123,\n",
            "         2011,  1996,  5436,  2059,  2339,  2025,  1029,  1026,  7987,  1013,\n",
            "         1028,  1026,  7987,  1013,  1028,  2021,  2008,  1005,  1055,  1996,\n",
            "         2919,  2112,  1010,  2045,  1005,  1055,  3294,  2053,  2342,  2005,\n",
            "         2009,  1012,  2044,  1037,  3056,  2391,  2009,  1005,  1055,  2035,\n",
            "         8040, 29122,  2075,  2007,  3294,  2053,  3574,  1012,  1006,  2182,\n",
            "         3310,  1996, 27594,  2121,  1007,  1012,  2044,  1996,  3199,  3496,\n",
            "        21982,  1998,  2010, 16222, 25377, 29146,  2525,  2031,  1996,  2769,\n",
            "         1012,  1045,  2481,  1005,  1056,  3305,  1996,  2342,  2005,  1996,\n",
            "         2717,  1997,  1996,  8040,  3286,  1012,  2003,  2009,  2035,  4072,\n",
            "         2074,  2000, 14548, 20493,  1005,  1055,  4451,  1999,  1996,  2755,\n",
            "         2008,  2002,  1005,  1055,  2042, 25857,  1029,  1045,  2123,  1005,\n",
            "         1056,  4965,  2009,  1012,  1026,  7987,  1013,  1028,  1026,  7987,\n",
            "         1013,  1028,  2061,  1020,  2041,  1997,  2184,  2005,  1017,  1013,\n",
            "         1018,  1997,  1996,  2143,  1998,  1016,  2041,  1997,  2184,  2005,\n",
            "         1996,  4566,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_ids = test_input_ids[:200]\n",
        "test_attention_masks = test_attention_masks[:200]\n",
        "test_labels = test_labels[:200]"
      ],
      "metadata": {
        "id": "yFi0JRvkUEBn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: how to get size of test_input_ids, test_attention_masks, test_labels\n",
        "\n",
        "print(f\"Size of test_input_ids: {test_input_ids.size()}\")\n",
        "print(f\"Size of test_attention_masks: {test_attention_masks.size()}\")\n",
        "print(f\"Size of test_labels: {test_labels.size()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00nql9IwUI4i",
        "outputId": "226bfa99-0c1a-4738-97df-4e51030d24eb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of test_input_ids: torch.Size([200, 512])\n",
            "Size of test_attention_masks: torch.Size([200, 512])\n",
            "Size of test_labels: torch.Size([200])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "batch_size = 16\n",
        "\n",
        "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
        "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            sampler = RandomSampler(train_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "val_dataloader = DataLoader(\n",
        "            val_dataset,\n",
        "            sampler = SequentialSampler(val_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "            test_dataset,\n",
        "            sampler = SequentialSampler(test_dataset),\n",
        "            batch_size = batch_size\n",
        "        )"
      ],
      "metadata": {
        "id": "NoHD1C2fuirB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "vgRyW8CpaRK4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict\n",
        "for batch in test_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  # Telling the model not to compute or store gradients, saving memory and\n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions.\n",
        "      result = model(b_input_ids,\n",
        "                     token_type_ids=None,\n",
        "                     attention_mask=b_input_mask,\n",
        "                     return_dict=True)\n",
        "\n",
        "  logits = result.logits\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "prediction_set = []\n",
        "\n",
        "for i in range(len(true_labels)):\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  prediction_set.append(pred_labels_i)\n",
        "\n",
        "prediction_scores = [item for sublist in prediction_set for item in sublist]\n",
        "\n",
        "print('Preds are ready')"
      ],
      "metadata": {
        "id": "O8z9MsApumf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7036845-e644-435b-f143-4a918251e5c0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preds are ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHoetkPAabsV",
        "outputId": "14e6dbb8-c29d-4c80-ad47-433f14db11fa"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([200])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1_score = f1_score(test_labels, prediction_scores, average='macro')\n",
        "print(f1_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYGvO4_iag8A",
        "outputId": "da3d6eef-e863-4279-935d-c4a7ee359ee2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9548634619724667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification"
      ],
      "metadata": {
        "id": "0IgEZj2fUZ4Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "WLZRXK7-UdK3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX3HdPI0UiID",
        "outputId": "aea8299f-1605-4179-f400-86d77b302c3d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGztPMbZUjqk",
        "outputId": "654a2ebf-6b61-4d09-8a2f-3be36a6ebded"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2YHCgcgSaHcs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}